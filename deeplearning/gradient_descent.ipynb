{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "通常需要初始化几个变量：\n",
    "\n",
    "- 学习率（learning_rate）\n",
    "学习率是控制梯度下降幅度的参数，亦称步长，学习率设置过大会阻碍收敛并导致损失函数在最小值附近波动甚至发散；学习率太小又会导致收敛速度缓慢，尤其是在迭代后期，当梯度变动很小的时候，整个收敛过程会变得很缓慢\n",
    "\n",
    "- 初始权重（theta）\n",
    "初始权重的个数等于原始样本中特征值的个数加1，其中新增的1个参数主要考虑偏置项()带来的影响\n",
    "\n",
    "- 程序终止条件（max_iteration_number / tolerance）\n",
    "    - 最大迭代次数：防止结果不收敛时，对程序进行强制终止\n",
    "    - 误差容忍度：当结果改善的变动低于某个阈值时，程序提前终止"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BatchGradientDescent:\n",
    "    def __init__(self, eta=0.01, n_iter=1000, tolerance=0.001):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        X = np.c_[np.ones(n_samples), X]  # 增加截距项\n",
    "        n_features = X.shape[-1]\n",
    "\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            errors = X.dot(self.theta) - y\n",
    "            loss = 1 / (2 * n_samples) * errors.dot(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "            else:\n",
    "                gradient = 1 / n_samples * X.T.dot(errors)\n",
    "                self.theta -= self.eta * gradient\n",
    "\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class StochasticGradientDescent(BatchGradientDescent):\n",
    "    def __init__(self, shuffle=True, random_state=None, **kwargs):\n",
    "        super(StochasticGradientDescent, self).__init__(**kwargs)\n",
    "        self.shuffle = shuffle\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)  # 重新排序\n",
    "            errors = []\n",
    "            for xi, yi in zip(X, y):\n",
    "                error_i = xi.dot(self.theta) - yi\n",
    "                errors.append(error_i ** 2)\n",
    "                gradient_i = xi.T.dot(error_i)  # 单个样本的梯度\n",
    "                self.theta -= self.eta * gradient_i\n",
    "            loss = 1 / 2 * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _shuffle(X, y):\n",
    "        location = np.random.permutation(len(y))\n",
    "        return X[location], y[location]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T02:36:43.166757Z",
     "start_time": "2023-05-06T02:36:43.135133100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class MiniBatchGradientDescent(StochasticGradientDescent):\n",
    "    def __init__(self, batch_size=10, **kwargs):\n",
    "        self.batch_size = batch_size\n",
    "        super(MiniBatchGradientDescent, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(len(X)), X]\n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.ones(n_features)\n",
    "        self.loss_ = [0]\n",
    "\n",
    "        self.i = 0\n",
    "        while self.i < self.n_iter:\n",
    "            self.i += 1\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "\n",
    "            errors = []\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "                mini_X, mini_y = X[j: j + self.batch_size], y[j: j + self.batch_size]\n",
    "                error = mini_X.dot(self.theta) - mini_y  # 长度与batch_size的长度一致\n",
    "                errors.append(error.dot(error))\n",
    "                mini_gradient = 1 / self.batch_size * mini_X.T.dot(error)  # 小批量样本梯度\n",
    "                self.theta -= self.eta * mini_gradient\n",
    "            loss = 1 / (2 * self.batch_size) * np.mean(errors)\n",
    "            delta_loss = loss - self.loss_[-1]\n",
    "            self.loss_.append(loss)\n",
    "            if np.abs(delta_loss) < self.tolerance:\n",
    "                break\n",
    "\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T02:38:01.937040800Z",
     "start_time": "2023-05-06T02:38:01.934871700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
