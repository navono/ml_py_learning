{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:56:12.179494Z",
     "start_time": "2023-05-17T07:56:12.177009200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 130.35092163085938, weight = [5.7978644], bias = [-1.22054]\n",
      "Epoch 1: loss = 80.10446166992188, weight = [0.8131633], bias = [-0.57643205]\n",
      "Epoch 2: loss = 49.238548278808594, weight = [4.7092514], bias = [-0.0611456]\n",
      "Epoch 3: loss = 30.273536682128906, weight = [1.6640334], bias = [0.3510836]\n",
      "Epoch 4: loss = 18.618064880371094, weight = [4.0442038], bias = [0.6808669]\n",
      "Epoch 5: loss = 11.45310115814209, weight = [2.1838412], bias = [0.94469357]\n",
      "Epoch 6: loss = 7.047455310821533, weight = [3.6379175], bias = [1.1557549]\n",
      "Epoch 7: loss = 4.337777614593506, weight = [2.501398], bias = [1.324604]\n",
      "Epoch 8: loss = 2.6707351207733154, weight = [3.389712], bias = [1.4596832]\n",
      "Epoch 9: loss = 1.6448556184768677, weight = [2.6953974], bias = [1.5677465]\n",
      "Epoch 10: loss = 1.0133565664291382, weight = [3.2380805], bias = [1.6541972]\n",
      "Epoch 11: loss = 0.6245081424713135, weight = [2.813914], bias = [1.7233578]\n",
      "Epoch 12: loss = 0.38499850034713745, weight = [3.1454465], bias = [1.7786863]\n",
      "Epoch 13: loss = 0.2374264895915985, weight = [2.8863177], bias = [1.822949]\n",
      "Epoch 14: loss = 0.1464715600013733, weight = [3.088855], bias = [1.8583592]\n",
      "Epoch 15: loss = 0.09039290249347687, weight = [2.9305503], bias = [1.8866874]\n",
      "Epoch 16: loss = 0.05580555275082588, weight = [3.0542824], bias = [1.9093499]\n",
      "Epoch 17: loss = 0.034465715289115906, weight = [2.9575725], bias = [1.92748]\n",
      "Epoch 18: loss = 0.02129450999200344, weight = [3.0331619], bias = [1.941984]\n",
      "Epoch 19: loss = 0.013162095099687576, weight = [2.9740803], bias = [1.9535873]\n",
      "Epoch 20: loss = 0.008138861507177353, weight = [3.0202591], bias = [1.9628699]\n",
      "Epoch 21: loss = 0.005034798290580511, weight = [2.9841652], bias = [1.9702959]\n",
      "Epoch 22: loss = 0.003115967381745577, weight = [3.0123768], bias = [1.9762367]\n",
      "Epoch 23: loss = 0.0019292684737592936, weight = [2.9903262], bias = [1.9809893]\n",
      "Epoch 24: loss = 0.0011950606713071465, weight = [3.0075612], bias = [1.9847915]\n",
      "Epoch 25: loss = 0.0007405757787637413, weight = [2.9940903], bias = [1.9878333]\n",
      "Epoch 26: loss = 0.0004591468605212867, weight = [3.0046191], bias = [1.9902667]\n",
      "Epoch 27: loss = 0.0002847992582246661, weight = [2.9963896], bias = [1.9922134]\n",
      "Epoch 28: loss = 0.0001767511566868052, weight = [3.002822], bias = [1.9937707]\n",
      "Epoch 29: loss = 0.00010974022006848827, weight = [2.9977944], bias = [1.9950166]\n",
      "Epoch 30: loss = 6.817082612542436e-05, weight = [3.001724], bias = [1.9960133]\n",
      "Epoch 31: loss = 4.236819950165227e-05, weight = [2.9986527], bias = [1.9968107]\n",
      "Epoch 32: loss = 2.634190968819894e-05, weight = [3.001053], bias = [1.9974486]\n",
      "Epoch 33: loss = 1.6388017684221268e-05, weight = [2.999177], bias = [1.9979588]\n",
      "Epoch 34: loss = 1.0200499673373997e-05, weight = [3.0006433], bias = [1.998367]\n",
      "Epoch 35: loss = 6.351596312015317e-06, weight = [2.9994974], bias = [1.9986936]\n",
      "Epoch 36: loss = 3.957014541811077e-06, weight = [3.000393], bias = [1.9989549]\n",
      "Epoch 37: loss = 2.467462081767735e-06, weight = [2.999693], bias = [1.999164]\n",
      "Epoch 38: loss = 1.5392186014651088e-06, weight = [3.00024], bias = [1.9993312]\n",
      "Epoch 39: loss = 9.603451189832413e-07, weight = [2.9998124], bias = [1.999465]\n",
      "Epoch 40: loss = 5.998830374664976e-07, weight = [3.0001466], bias = [1.999572]\n",
      "Epoch 41: loss = 3.7467154356818355e-07, weight = [2.9998856], bias = [1.9996576]\n",
      "Epoch 42: loss = 2.3400288284847193e-07, weight = [3.0000896], bias = [1.999726]\n",
      "Epoch 43: loss = 1.4655242353001086e-07, weight = [2.99993], bias = [1.9997808]\n",
      "Epoch 44: loss = 9.19290243928117e-08, weight = [3.0000548], bias = [1.9998246]\n",
      "Epoch 45: loss = 5.756949761348551e-08, weight = [2.999957], bias = [1.9998597]\n",
      "Epoch 46: loss = 3.611055632291027e-08, weight = [3.0000336], bias = [1.9998877]\n",
      "Epoch 47: loss = 2.2670189281370767e-08, weight = [2.9999738], bias = [1.9999102]\n",
      "Epoch 48: loss = 1.420207063063117e-08, weight = [3.0000205], bias = [1.9999282]\n",
      "Epoch 49: loss = 8.882341084870404e-09, weight = [2.999984], bias = [1.9999425]\n",
      "Epoch 50: loss = 5.582212470045533e-09, weight = [3.0000126], bias = [1.999954]\n",
      "Epoch 51: loss = 3.518062507978925e-09, weight = [2.9999902], bias = [1.9999632]\n",
      "Epoch 52: loss = 2.2291051227085745e-09, weight = [3.0000076], bias = [1.9999706]\n",
      "Epoch 53: loss = 1.3836304324499338e-09, weight = [2.999994], bias = [1.9999765]\n",
      "Epoch 54: loss = 8.794127093203485e-10, weight = [3.0000048], bias = [1.9999813]\n",
      "Epoch 55: loss = 5.554142146202423e-10, weight = [2.9999962], bias = [1.9999851]\n",
      "Epoch 56: loss = 3.590959751775813e-10, weight = [3.000003], bias = [1.9999881]\n",
      "Epoch 57: loss = 2.26034538308717e-10, weight = [2.9999976], bias = [1.9999905]\n",
      "Epoch 58: loss = 1.4284182170420934e-10, weight = [3.000002], bias = [1.9999924]\n",
      "Epoch 59: loss = 8.955348962391696e-11, weight = [2.9999986], bias = [1.9999939]\n",
      "Epoch 60: loss = 5.49505337066325e-11, weight = [3.0000012], bias = [1.9999951]\n",
      "Epoch 61: loss = 3.507712523220796e-11, weight = [2.9999993], bias = [1.9999961]\n",
      "Epoch 62: loss = 2.029025905625481e-11, weight = [3.0000007], bias = [1.9999969]\n",
      "Epoch 63: loss = 1.3019985263540423e-11, weight = [2.9999995], bias = [1.9999975]\n",
      "Epoch 64: loss = 9.102052445086883e-12, weight = [3.0000005], bias = [1.999998]\n",
      "Epoch 65: loss = 5.5550230214362895e-12, weight = [2.9999998], bias = [1.9999983]\n",
      "Epoch 66: loss = 3.892353019757655e-12, weight = [3.0000002], bias = [1.9999987]\n",
      "Epoch 67: loss = 1.9525713511325815e-12, weight = [2.9999998], bias = [1.9999989]\n",
      "Epoch 68: loss = 1.558457030960414e-12, weight = [3.0000002], bias = [1.9999992]\n",
      "Epoch 69: loss = 1.323504240230422e-12, weight = [2.9999998], bias = [1.9999994]\n",
      "Epoch 70: loss = 1.2628713179363427e-12, weight = [3.0000002], bias = [1.9999995]\n",
      "Epoch 71: loss = 7.811233214069513e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 72: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 73: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 74: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 75: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 76: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 77: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 78: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 79: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 80: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 81: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 82: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 83: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 84: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 85: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 86: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 87: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 88: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 89: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 90: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 91: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 92: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 93: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 94: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 95: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 96: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 97: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n",
      "Epoch 98: loss = 6.26224983327206e-13, weight = [3.0000002], bias = [1.9999996]\n",
      "Epoch 99: loss = 5.428546609627383e-13, weight = [2.9999998], bias = [1.9999996]\n"
     ]
    }
   ],
   "source": [
    "# 一元线性方程\n",
    "x = torch.linspace(-5, 5, 30)\n",
    "real_y = x * 3 + 2\n",
    "weight = torch.randn(1).requires_grad_(True)\n",
    "bias = torch.randn(1).requires_grad_(True)\n",
    "\n",
    "opt = torch.optim.SGD(params=[weight, bias], lr=1e-1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    pred_y = x * weight + bias\n",
    "    loss = loss_func(pred_y, real_y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss = {loss.data.numpy()}, weight = {weight.data.numpy()}, bias = {bias.data.numpy()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:50:23.789532800Z",
     "start_time": "2023-05-17T07:50:23.753752100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 0: loss = 103.67320251464844\n",
      "Epoch 0, step 1: loss = 375.0393371582031\n",
      "Epoch 0, step 2: loss = 265528.3125\n",
      "Epoch 0, step 3: loss = 318699995136000.0\n",
      "Epoch 0, step 4: loss = inf\n",
      "Epoch 1, step 0: loss = inf\n",
      "Epoch 1, step 1: loss = nan\n",
      "Epoch 1, step 2: loss = nan\n",
      "Epoch 1, step 3: loss = nan\n",
      "Epoch 1, step 4: loss = nan\n",
      "Epoch 2, step 0: loss = nan\n",
      "Epoch 2, step 1: loss = nan\n",
      "Epoch 2, step 2: loss = nan\n",
      "Epoch 2, step 3: loss = nan\n",
      "Epoch 2, step 4: loss = nan\n",
      "Epoch 3, step 0: loss = nan\n",
      "Epoch 3, step 1: loss = nan\n",
      "Epoch 3, step 2: loss = nan\n",
      "Epoch 3, step 3: loss = nan\n",
      "Epoch 3, step 4: loss = nan\n",
      "Epoch 4, step 0: loss = nan\n",
      "Epoch 4, step 1: loss = nan\n",
      "Epoch 4, step 2: loss = nan\n",
      "Epoch 4, step 3: loss = nan\n",
      "Epoch 4, step 4: loss = nan\n",
      "Epoch 5, step 0: loss = nan\n",
      "Epoch 5, step 1: loss = nan\n",
      "Epoch 5, step 2: loss = nan\n",
      "Epoch 5, step 3: loss = nan\n",
      "Epoch 5, step 4: loss = nan\n",
      "Epoch 6, step 0: loss = nan\n",
      "Epoch 6, step 1: loss = nan\n",
      "Epoch 6, step 2: loss = nan\n",
      "Epoch 6, step 3: loss = nan\n",
      "Epoch 6, step 4: loss = nan\n",
      "Epoch 7, step 0: loss = nan\n",
      "Epoch 7, step 1: loss = nan\n",
      "Epoch 7, step 2: loss = nan\n",
      "Epoch 7, step 3: loss = nan\n",
      "Epoch 7, step 4: loss = nan\n",
      "Epoch 8, step 0: loss = nan\n",
      "Epoch 8, step 1: loss = nan\n",
      "Epoch 8, step 2: loss = nan\n",
      "Epoch 8, step 3: loss = nan\n",
      "Epoch 8, step 4: loss = nan\n",
      "Epoch 9, step 0: loss = nan\n",
      "Epoch 9, step 1: loss = nan\n",
      "Epoch 9, step 2: loss = nan\n",
      "Epoch 9, step 3: loss = nan\n",
      "Epoch 9, step 4: loss = nan\n",
      "Epoch 10, step 0: loss = nan\n",
      "Epoch 10, step 1: loss = nan\n",
      "Epoch 10, step 2: loss = nan\n",
      "Epoch 10, step 3: loss = nan\n",
      "Epoch 10, step 4: loss = nan\n",
      "Epoch 11, step 0: loss = nan\n",
      "Epoch 11, step 1: loss = nan\n",
      "Epoch 11, step 2: loss = nan\n",
      "Epoch 11, step 3: loss = nan\n",
      "Epoch 11, step 4: loss = nan\n",
      "Epoch 12, step 0: loss = nan\n",
      "Epoch 12, step 1: loss = nan\n",
      "Epoch 12, step 2: loss = nan\n",
      "Epoch 12, step 3: loss = nan\n",
      "Epoch 12, step 4: loss = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 26\u001B[0m\n\u001B[0;32m     23\u001B[0m loss_func \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mMSELoss()\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[1;32m---> 26\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m step, (batch_x, batch_y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(loader):\n\u001B[0;32m     27\u001B[0m         output \u001B[38;5;241m=\u001B[39m net(batch_x)\n\u001B[0;32m     28\u001B[0m         loss \u001B[38;5;241m=\u001B[39m loss_func(output, batch_y)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1328\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1329\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1331\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1290\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1291\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1292\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1293\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1294\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1295\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1296\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[0;32m   1120\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[0;32m   1121\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[0;32m   1130\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[0;32m   1131\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1132\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[0;32m   1134\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1135\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[0;32m   1137\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\multiprocessing\\queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[0;32m    112\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[1;32m--> 113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\multiprocessing\\connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[1;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\multiprocessing\\connection.py:330\u001B[0m, in \u001B[0;36mPipeConnection._poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_got_empty_message \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m    328\u001B[0m             _winapi\u001B[38;5;241m.\u001B[39mPeekNamedPipe(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 330\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\multiprocessing\\connection.py:879\u001B[0m, in \u001B[0;36mwait\u001B[1;34m(object_list, timeout)\u001B[0m\n\u001B[0;32m    876\u001B[0m                 ready_objects\u001B[38;5;241m.\u001B[39madd(o)\n\u001B[0;32m    877\u001B[0m                 timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 879\u001B[0m     ready_handles \u001B[38;5;241m=\u001B[39m \u001B[43m_exhaustive_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaithandle_to_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;66;03m# request that overlapped reads stop\u001B[39;00m\n\u001B[0;32m    882\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ov \u001B[38;5;129;01min\u001B[39;00m ov_list:\n",
      "File \u001B[1;32m~\\.conda\\envs\\py310-ml\\lib\\multiprocessing\\connection.py:811\u001B[0m, in \u001B[0;36m_exhaustive_wait\u001B[1;34m(handles, timeout)\u001B[0m\n\u001B[0;32m    809\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    810\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m L:\n\u001B[1;32m--> 811\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43m_winapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWaitForMultipleObjects\u001B[49m\u001B[43m(\u001B[49m\u001B[43mL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    812\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m WAIT_TIMEOUT:\n\u001B[0;32m    813\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 仅作演示\n",
    "\n",
    "x = torch.linspace(-5, 5, 500).view(-1, 1)\n",
    "y = x ** 2\n",
    "y += torch.normal(0, 0.01, x.size())  # 加上噪音数据\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = torch.nn.Linear(n_in, n_hidden)\n",
    "        self.layer_2 = torch.nn.Linear(n_hidden, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "loader = DataLoader(dataset=dataset, batch_size=100, shuffle=True, num_workers=2)\n",
    "net = Net(n_in=1, n_hidden=10, n_out=1)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        output = net(batch_x)\n",
    "        loss = loss_func(output, batch_y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        print('Epoch {}, step {}: loss = {}' \\\n",
    "              .format(epoch, step, loss.data.numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T07:56:47.403396100Z",
     "start_time": "2023-05-17T07:56:23.919588500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
