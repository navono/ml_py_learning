{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "训练模型是个迭代过程，在每个迭代（也称之为 `epoch`）中，该模型对输出进行猜测，计算其猜测中的误差（损失），收集误差相对于其参数的导数（如我们在前面的模块中看到的），并使用梯度下降优化这些参数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:03:38.078187200Z",
     "start_time": "2023-06-09T08:03:38.058580100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-09T06:51:31.393185400Z",
     "start_time": "2023-06-09T06:51:31.336132900Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`超参` 指的是那些在训练过程中明确设置的参数。这些参数用于控制训练过程本身的行为，而不是模型内部的参数值。我们在本文中设置的超参数如下：\n",
    "- `Epoch` 的数量——训练数据集上迭代的次数。\n",
    "- `Batch size`——每次迭代中传递给模型的数据样本数量。\n",
    "- `Learning rate`——优化器内部使用的更新模型参数的大小。较大的值可能会导致收敛过程中的振荡，较小的值可能会导致收敛速度过慢。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T06:53:10.422595600Z",
     "start_time": "2023-06-09T06:53:10.406440600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "损失函数的作用：\n",
    "- 损失函数衡量得到的结果与目标值的不同程度\n",
    "- 损失函数的梯度有助于优化器在训练期间进行适当的参数调整\n",
    "- 损失函数是在训练过程中以最小化为目标"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:02:58.057224300Z",
     "start_time": "2023-06-09T08:02:58.052385700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:03:41.941506100Z",
     "start_time": "2023-06-09T08:03:41.927130300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在迭代训练中，通常由三步构成：\n",
    "- 调用 `optimizer.zero_grad()` 重置模型参数的梯度。默认情况下，每次调用 `.backward()` ，梯度都会累积（即不会被重写）。\n",
    "- 调用 `loss.backward()` 计算当前迭代的损失相对于模型参数的梯度。\n",
    "- 调用 `optimizer.step()` 更新内部参数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:05:38.099704200Z",
     "start_time": "2023-06-09T08:05:38.091801900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301069  [    0/60000]\n",
      "loss: 2.293265  [ 6400/60000]\n",
      "loss: 2.280343  [12800/60000]\n",
      "loss: 2.277944  [19200/60000]\n",
      "loss: 2.250764  [25600/60000]\n",
      "loss: 2.233701  [32000/60000]\n",
      "loss: 2.233867  [38400/60000]\n",
      "loss: 2.212499  [44800/60000]\n",
      "loss: 2.222494  [51200/60000]\n",
      "loss: 2.175834  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 0.034337 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.216313  [    0/60000]\n",
      "loss: 2.205490  [ 6400/60000]\n",
      "loss: 2.171623  [12800/60000]\n",
      "loss: 2.171337  [19200/60000]\n",
      "loss: 2.096050  [25600/60000]\n",
      "loss: 2.073166  [32000/60000]\n",
      "loss: 2.085468  [38400/60000]\n",
      "loss: 2.033377  [44800/60000]\n",
      "loss: 2.060772  [51200/60000]\n",
      "loss: 1.977472  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 0.031318 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.053993  [    0/60000]\n",
      "loss: 2.021694  [ 6400/60000]\n",
      "loss: 1.945488  [12800/60000]\n",
      "loss: 1.963649  [19200/60000]\n",
      "loss: 1.826245  [25600/60000]\n",
      "loss: 1.793093  [32000/60000]\n",
      "loss: 1.827798  [38400/60000]\n",
      "loss: 1.732156  [44800/60000]\n",
      "loss: 1.784967  [51200/60000]\n",
      "loss: 1.696800  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.026846 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.807049  [    0/60000]\n",
      "loss: 1.757328  [ 6400/60000]\n",
      "loss: 1.647813  [12800/60000]\n",
      "loss: 1.684924  [19200/60000]\n",
      "loss: 1.529951  [25600/60000]\n",
      "loss: 1.507147  [32000/60000]\n",
      "loss: 1.549850  [38400/60000]\n",
      "loss: 1.450266  [44800/60000]\n",
      "loss: 1.509539  [51200/60000]\n",
      "loss: 1.449921  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.022969 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.583791  [    0/60000]\n",
      "loss: 1.537211  [ 6400/60000]\n",
      "loss: 1.412806  [12800/60000]\n",
      "loss: 1.470622  [19200/60000]\n",
      "loss: 1.317960  [25600/60000]\n",
      "loss: 1.305333  [32000/60000]\n",
      "loss: 1.353812  [38400/60000]\n",
      "loss: 1.269781  [44800/60000]\n",
      "loss: 1.324154  [51200/60000]\n",
      "loss: 1.294372  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.020433 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:06:42.970830500Z",
     "start_time": "2023-06-09T08:06:13.595395900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"data/model.pth\")\n",
    "\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T08:07:20.128386400Z",
     "start_time": "2023-06-09T08:07:20.086917500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
